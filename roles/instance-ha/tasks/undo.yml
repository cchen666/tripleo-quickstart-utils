---
- name: Get the name of the stack
  shell: |
    source {{ working_dir }}/stackrc
    openstack stack list -f value -c 'Stack Name'
  register: stack_name
  delegate_to: "{{ groups['undercloud'][0] }}"

- name: Get the contents of the overcloud's rc file
  shell: cat {{ working_dir }}/{{ stack_name.stdout }}rc
  register: overcloudrc

- block:
  - name: Remove overcloudrc file on first controller (if exists)
    file:
      path: "{{ overcloud_working_dir }}/overcloudrc"
      state: absent

  - name: Copy overcloudrc file on first controller
    lineinfile:
      destfile: "{{ overcloud_working_dir }}/overcloudrc"
      line: "{{ overcloudrc.stdout }}"
      create: yes
      mode: 0644

  - name: Get environment vars from overcloudrc
    shell: >
       grep OS_USERNAME {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_USERNAME=//g'
    register: "OS_USERNAME"

  - name: Get environment vars from overcloudrc
    shell: >
       grep OS_PASSWORD {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_PASSWORD=//g'
    register: "OS_PASSWORD"

  - name: Get environment vars from overcloudrc
    shell: >
       grep OS_AUTH_URL {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_AUTH_URL=//g'
    register: "OS_AUTH_URL"

  - name: Get environment vars from overcloudrc
    shell: >
       grep -E 'OS_PROJECT_NAME|OS_TENANT_NAME' {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_.*_NAME=//g'
    register: "OS_TENANT_NAME"
  delegate_to: "{{ groups.controller[0] }}"

- block:
    - name: Remove fence-nova STONITH device
      shell: |
          pcs stonith delete fence-nova

    - name: Remove resources associated to remote nodes
      shell: |
          for resourceid in $(pcs resource show | grep compute | grep -v -e Stopped: -e Started: -e disabled -e remote | awk '{print $3}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

    - name: Remove NovaEvacuate resource
      shell: |
          for resourceid in $(pcs resource show | grep NovaEvacuate | awk '{print $1}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

    - name: Remove pacemaker remote resource
      shell: |
          for resourceid in $(pcs resource show | grep :remote | awk '{print $1}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

#    - name: Erase the status entries corresponding to the compute nodes
#      shell: |
#          cibadmin --delete --xml-text "<node id='{{ item }}'/>"
#          cibadmin --delete --xml-text "<node_state id={{ item }}'/>"
#      with_items:
#        - "{{ groups['compute'] }}"

    - name: Remove constraints related to role controller
      shell: |
          for constraintid in $(pcs config show | grep -B 3 "osprole eq controller" | awk '/Constraint/ {print $2}')
           do
            pcs constraint delete $constraintid
           done

    - name: Unset controller pacemaker property on controllers
      shell: "pcs property unset --node {{ hostvars[item]['ansible_hostname'] }} osprole"
      with_items: "{{ groups['controller'] }}"

    - name: Unset cluster recheck interval to 1 minute
      shell: "pcs property unset cluster-recheck-interval"

  environment:
     OS_USERNAME: "{{ OS_USERNAME.stdout }}"
     OS_PASSWORD: "{{ OS_PASSWORD.stdout }}"
     OS_AUTH_URL: "{{ OS_AUTH_URL.stdout }}"
     OS_TENANT_NAME: "{{ OS_TENANT_NAME.stdout }}"
  become: yes
  delegate_to: "{{ groups.controller[0] }}"

- name: Cleanup failed resources (if any)
  shell: |
     for resource in $(pcs status | sed -n -e '/Failed Actions:/,/^$/p' | egrep 'OCF_|not running|unknown' | awk '{print $2}' | cut -f1 -d_ | sort |uniq)
      do
       pcs resource cleanup $resource
      done
  become: yes
  delegate_to: "{{ groups.controller[0] }}"

- name: Wait for failed resources to recover (if any)
  shell: pcs status | sed -n -e '/Failed Actions:/,/^$/p' | egrep 'OCF_|not running|unknown' | awk '{print $2}' | cut -f1 -d_ | sort |uniq
  register: failed_resources
  until: failed_resources.stdout != []
  retries: 10
  delay: 10
  become: yes
  delegate_to: "{{ groups.controller[0] }}"

- name: Enable openstack-nova-compute on compute
  service:
    name: openstack-nova-compute
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['compute'] }}"

- name: Enable neutron-openvswitch-agent on compute
  service:
    name: neutron-openvswitch-agent
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release in [ 'liberty', 'rhos-8', 'mitaka', 'rhos-9' ]

- name: Enable neutron-openvswitch-agent on compute
  service:
    name: openstack-ceilometer-compute
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release in [ 'liberty', 'rhos-8', 'mitaka', 'rhos-9' ]

- name: Enable libvirtd on compute
  become: yes
  service:
    name: libvirtd
    state: started
    enabled: yes
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release in [ 'liberty', 'rhos-8', 'mitaka', 'rhos-9' ]

- name: Stop pacemaker remote service on compute nodes
  become: yes
  service:
    name: pacemaker_remote
    enabled: no
    state: stopped
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['compute'] }}"

- name: Disable iptables traffic for pacemaker_remote
  become: yes
  shell: >
    iptables -D INPUT -p tcp --dport 3121 -j ACCEPT;
    /sbin/service iptables save
  delegate_to: "{{ hostvars[item]['ansible_hostname'] }}"
  with_items:
    - "{{ groups['controller'] }}"
    - "{{ groups['compute'] }}"

- name: Undo STONITH for compute nodes
  include_role:
    name: stonith-config
  vars:
    stonith_action: "uninstall"
    stonith_devices: "computes"
  when:
    - stonith_devices in ["all","computes"]
