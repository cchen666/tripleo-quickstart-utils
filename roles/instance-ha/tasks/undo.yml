---
- name: Get the overcloudrc file
  shell: >
    cat {{ working_dir }}/overcloudrc
  register: overcloudrc

- name: Copy overcloudrc file on overcloud-controller-0
  lineinfile:
    destfile: "{{ overcloud_working_dir }}/overcloudrc"
    line: "{{ overcloudrc.stdout }}"
    create: yes
    mode: 0644
  delegate_to: overcloud-controller-0

- name: Get environment vars from overcloudrc
  delegate_to: "overcloud-controller-0"
  shell: >
     grep OS_USERNAME {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_USERNAME=//g'
  register: "OS_USERNAME"

- name: Get environment vars from overcloudrc
  delegate_to: "overcloud-controller-0"
  shell: >
     grep OS_PASSWORD {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_PASSWORD=//g'
  register: "OS_PASSWORD"

- name: Get environment vars from overcloudrc
  delegate_to: "overcloud-controller-0"
  shell: >
     grep OS_AUTH_URL {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_AUTH_URL=//g'
  register: "OS_AUTH_URL"

- name: Get environment vars from overcloudrc
  delegate_to: "overcloud-controller-0"
  shell: >
     grep -E 'OS_PROJECT_NAME|OS_TENANT_NAME' {{ overcloud_working_dir }}/overcloudrc | sed 's/export OS_.*_NAME=//g'
  register: "OS_TENANT_NAME"

- block:
    - name: Remove fence-nova STONITH device
      shell: |
          pcs stonith delete fence-nova

    - name: Remove resources associated to remote nodes
      shell: |
          for resourceid in $(pcs resource show | grep compute | grep -v -e Stopped: -e Started: -e disabled -e remote | awk '{print $3}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

    - name: Remove NovaEvacuate resource
      shell: |
          for resourceid in $(pcs resource show | grep NovaEvacuate | awk '{print $1}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

    - name: Remove pacemaker remote resource
      shell: |
          for resourceid in $(pcs resource show | grep :remote | awk '{print $1}')
           do
            pcs resource cleanup $resourceid
            pcs --force resource delete $resourceid
           done

    - name: Erase the status entries corresponding to the compute nodes
      shell: |
          cibadmin --delete --xml-text "<node id='{{ item }}'/>"
          cibadmin --delete --xml-text "<node_state id={{ item }}'/>"
      with_items:
        - "{{ groups['compute'] }}"

    - name: Delete pacemaker compute remote resource
      shell: pcs resource delete {{ item }}
      with_items: "{{ groups['compute'] }}"

    - name: Delete pacemaker compute remote node
      shell: crm_node --force --remove {{ item }}
      with_items: "{{ groups['compute'] }}"

    - name: Delete resource nova-evacuate
      shell: >
        pcs resource delete nova-evacuate

    - name: Remove constraints related to role controller
      shell: |
          for constraintid in $(pcs config show | grep -B 3 "osprole eq controller" | awk '/Constraint/ {print $2}')
           do
            sudo pcs constraint delete $constraintid
           done

    - name: Unset controller pacemaker property on controllers
      shell: "pcs property unset --node {{ item }} osprole"
      with_items: "{{ groups['controller'] }}"

    - name: Unset cluster recheck interval to 1 minute
      shell: "pcs property unset cluster-recheck-interval"

  environment:
     OS_USERNAME: "{{ OS_USERNAME.stdout }}"
     OS_PASSWORD: "{{ OS_PASSWORD.stdout }}"
     OS_AUTH_URL: "{{ OS_AUTH_URL.stdout }}"
     OS_TENANT_NAME: "{{ OS_TENANT_NAME.stdout }}"
  become: yes
  delegate_to: "overcloud-controller-0"

- name: Cleanup failed resources (if any)
  shell: |
     for resource in $(pcs status | sed -n -e '/Failed Actions:/,/^$/p' | egrep 'OCF_|not running|unknown' | awk '{print $2}' | cut -f1 -d_ | sort |uniq)
      do
       pcs resource cleanup $resource
      done
  become: yes
  delegate_to: "overcloud-controller-0"

- name: Wait for failed resources to recover (if any)
  shell: pcs status | sed -n -e '/Failed Actions:/,/^$/p' | egrep 'OCF_|not running|unknown' | awk '{print $2}' | cut -f1 -d_ | sort |uniq
  register: failed_resources
  until: failed_resources.stdout != []
  retries: 10
  delay: 10
  become: yes
  delegate_to: "overcloud-controller-0"

- name: Enable openstack-nova-compute on compute
  service:
    name: openstack-nova-compute
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['compute'] }}"

- name: Enable neutron-openvswitch-agent on compute
  service:
    name: neutron-openvswitch-agent
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release == 'liberty' or release == 'mitaka'

- name: Enable neutron-openvswitch-agent on compute
  service:
    name: openstack-ceilometer-compute
    state: started
    enabled: yes
  become: yes
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release == 'liberty' or release == 'mitaka'

- name: Enable libvirtd on compute
  become: yes
  service:
    name: libvirtd
    state: started
    enabled: yes
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['compute'] }}"
  when: release == 'liberty' or release == 'mitaka'

- name: Stop pacemaker remote service on compute nodes
  become: yes
  service:
    name: pacemaker_remote
    enabled: no
    state: stopped
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['compute'] }}"

- name: Disable iptables traffic for pacemaker_remote
  become: yes
  shell: >
    iptables -D INPUT -p tcp --dport 3121 -j ACCEPT;
    /sbin/service iptables save
  delegate_to: "{{ item }}"
  with_items:
    - "{{ groups['controller'] }}"
    - "{{ groups['compute'] }}"
